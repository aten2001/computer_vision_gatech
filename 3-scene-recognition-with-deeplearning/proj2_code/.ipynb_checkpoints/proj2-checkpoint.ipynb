{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "id": "fiK4QNWKDvWJ",
    "outputId": "6d715883-9b4c-4ec6-aa1b-69257d5cea39"
   },
   "source": [
    "# Scene Recognition with Deep Learning\n",
    "\n",
    "## Brief\n",
    "- Due\n",
    "    - Part 1 Code 09/30/2020 11:59PM (this notebook)\n",
    "    - Part 2 Code 10/13/2020 11:59PM (to be released later)\n",
    "    - Report 10/13/2020 11:59PM (to be release later; submission along with Part 2)\n",
    "- Hand-in: through Gradescope\n",
    "- Required files: \n",
    "    - `<your_gt_username>.zip` against [Project 2 - Part 1 - Code](https://www.gradescope.com/courses/155064/assignments/676907)\n",
    "        \n",
    "**Note**:\n",
    "Working on pairs and individual submissions has to be decided for Project 2 as a whole. For part 1, the grading scheme and total points are the same for indivudual and pair submissions.\n",
    "    \n",
    "\n",
    "## Outline\n",
    "In this project, we will use *convolutional neural nets* to classify images into different scenes.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "1. Construct the fundamental pipeline for performing deep learning using PyTorch;\n",
    "2. Understand the concepts behind different layers, optimizers.\n",
    "3. Experiment with different models and observe the performance.\n",
    "\n",
    "The starter code is mostly initialized to 'placeholder' just so that the starter\n",
    "code does not crash when run unmodified and you can get a preview of how\n",
    "results are presented.\n",
    "\n",
    "## Compute Requirements\n",
    "\n",
    "This project is doable without a GPU, but a GPU makes the process much more faster and frustration free. Part 1 should be fine to do on a CPU with a little bit of patience.\n",
    "\n",
    "You can try out Google Colab to run this notebook. These are the steps we follow:\n",
    "1. Upload this notebook to google colab\n",
    "2. Zip all the components in the project directory and upload it to the colab runtime\n",
    "3. Unzip the uploaded zip using ```!unzip -qq <uploaded_file>.zip -d ./```\n",
    "\n",
    "Remember to download all the files saved and changes to code you made on the colab.\n",
    "\n",
    "Note that we will not be actively supporting issues with Google colab. Please take help from fellow students and use the internet to solve the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "8QRsVzCGTdjB",
    "outputId": "10d6258c-2ef5-4705-aa11-f0ad11c67577"
   },
   "outputs": [],
   "source": [
    "# uncomment for running on colab\n",
    "#!unzip -qq proj2_colab.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag to modify paths to run better on Colab; change it to true if you want to run on colab\n",
    "use_colab_paths = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3KVmDW-TdjI"
   },
   "source": [
    "## Dataset\n",
    "The dataset is in the ```data``` folder. It has two subfolders: ```train``` and ```test```. Go through any of the folder and find there you will find the folders with scene names like *bedroom*, *forest*, *office*. These are the 15 scenes that we want our model to predict given an RGB image. You can look into folder for each scene to find multiple images. All this data is labelled data provided to you for training and testing your model.\n",
    "\n",
    "**Let's start coding now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFVeSYKzTdjK"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "g1dqr6qSBpE2",
    "outputId": "9f3a3024-a162-49c9-eada-33e205d97deb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proj2_code.runner import Trainer\n",
    "from proj2_code.optimizer import get_optimizer\n",
    "from proj2_code.simple_net import SimpleNet\n",
    "from proj2_code.image_loader import ImageLoader\n",
    "from proj2_code.data_transforms import get_fundamental_transforms, get_data_augmentation_transforms\n",
    "from proj2_code.stats_helper import compute_mean_and_std\n",
    "from proj2_code.vis import visualize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CG3e0869TdjS"
   },
   "outputs": [],
   "source": [
    "from proj2_code.proj2_unit_tests.test_base import verify\n",
    "from proj2_code.proj2_unit_tests.test_stats_helper import test_mean_and_variance\n",
    "from proj2_code.proj2_unit_tests.test_image_loader import test_dataset_length, test_unique_vals, test_class_values, test_load_img_from_path\n",
    "from proj2_code.proj2_unit_tests.test_data_transforms import test_fundamental_transforms\n",
    "from proj2_code.proj2_unit_tests.test_dl_utils import test_predict_labels, test_compute_loss\n",
    "from proj2_code.proj2_unit_tests.test_simple_net import test_simple_net\n",
    "from proj2_code.proj2_unit_tests.test_checkpoints import test_simple_net_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjE0jIc5BpFN"
   },
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "is_cuda = is_cuda and torch.cuda.is_available() # will turn off cuda if the machine doesnt have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKRS4gvZTdji"
   },
   "outputs": [],
   "source": [
    "data_base_path = '../data/' if not use_colab_paths else 'data/'\n",
    "model_base_path = '../model_checkpoints/' if not use_colab_paths else 'model_checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "['simple_net']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(data_base_path))\n",
    "print(os.listdir(model_base_path))\n",
    "\n",
    "# TODO: check that these outputs are as per expectation. It will save a lot of time in debugging issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nulEn5fzTdjs"
   },
   "source": [
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B027mwlKTdjt"
   },
   "source": [
    "## 1 Datasets\n",
    "One crucial aspect of deep learning is to perform data preprocessing. In DL, we usually *normalize* the dataset and perform some *transformations* on them. The transformations can either help the inputs be compatible with the model (say our model only works on 500x500 images and we need all input to be cropped/scaled to this size) or help in data-augmentation to improve performance (more on this later).\n",
    "\n",
    "\n",
    "### 1.1 Compute mean and standard deviation of the dataset\n",
    "In this project we are going to \"zero-center\" and \"normalize\" the dataset so that each entry has zero mean and the overall standard deviation is 1. \n",
    "\n",
    "**TODO 1**:  fill in the `compute_mean_and_std()` in `stats_helper.py` to compute the **mean** and **standard deviation** of both training and validation data.\n",
    "\n",
    "Debug tip: If you face an error from StandardScaler about attribute not found, please check that the paths are correct and your code actually finds some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 576])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "data = torch.zeros(100, 3, 24, 24)\n",
    "batch_samples = data.size(0)\n",
    "data = data.view(batch_samples, data.size(1), -1)\n",
    "\n",
    "print(data.shape)\n",
    "print(data.mean(2).sum(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vWA_2UbjBpFd",
    "outputId": "7e8b1de9-2c63-472c-f7ee-c70fe31708fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual means [0.46178914] [0.256041]\n",
      "Testing your mean and std computation:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your mean and std computation: \", verify(test_mean_and_variance))\n",
    "dataset_mean, dataset_std = compute_mean_and_std(data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xixFr8CDBpFn",
    "outputId": "fe9015a7-a66a-4cc3-e062-6ba799d69d01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mean = [0.45547487], standard deviation = [0.25316328]\n"
     ]
    }
   ],
   "source": [
    "print('Dataset mean = {}, standard deviation = {}'.format(dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2TeGbrQBpFu"
   },
   "source": [
    "### 1.2 ImageLoader\n",
    "\n",
    "Now let's create the **Datasets** object to be used later. Remember back in Project 1, we have initialized such a class to load 5 images? Here the task is similar: we have to load each image as well as it's classification label. The essence is to retrieve the paths to all the images required, and be able to provide the **path** and the **class id** when given an index.\n",
    "\n",
    "We will map the scene names (text) into indices 0 to 14 in the image loader. You can choose any mapping you want but once fixed, it has to be consistent throughout this notebook.\n",
    "\n",
    "**TODO 2:** complete the `image_loader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "THRvAvluXFcS",
    "outputId": "36f9ce2a-2ec9-47cc-e5e9-411d29d04b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your image loader (length): \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your image loader (values): \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your image loader (classes): \u001b[32m\"Correct\"\u001b[0m\n",
      "Testing your image loader (paths): \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "inp_size = (64,64)\n",
    "print(\"Testing your image loader (length):\", verify(test_dataset_length))\n",
    "print(\"Testing your image loader (values):\", verify(test_unique_vals))\n",
    "print(\"Testing your image loader (classes):\", verify(test_class_values))\n",
    "print(\"Testing your image loader (paths):\", verify(test_load_img_from_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIq75D2CTdkC"
   },
   "source": [
    "### 1.3 Data transforms\n",
    "In this section, we will construct some fundamental transforms to process RGB images into torch tensors, which we can provide as input to our model.\n",
    "\n",
    "1. Resize the input image to the desired shape;\n",
    "2. Convert it to a tensor;\n",
    "3. Normalize them based on the computed mean and standard deviation.\n",
    "\n",
    "**TODO 3:** For this part, complete the function `get_fundamental_transforms()` in `data_transforms.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZoAaL11nTdkD",
    "outputId": "f2a7ad1b-77a2-4f5b-ff7b-90b1462a84de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your fundamental data transforms:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your fundamental data transforms: \", verify(test_fundamental_transforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sWOnZmNTdkJ"
   },
   "source": [
    "## 2 Model Architecture and Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bg59TFXHTdkL"
   },
   "source": [
    "### 2.1 SimpleNet Model\n",
    "\n",
    "The data is ready! Now we are preparing to move to the actual core of deep learning: the architecture. To get you started in this part, simply define a **2-layer** model in the `simple_net.py`. Here by \"2 layers\" we mean **2 convolutional layers**, so you need to figure out the supporting utilities like ReLU, Max Pooling, and Fully Connected layers, and configure them with proper parameters to make the tensor flow.\n",
    "\n",
    "You may refer the image *simplenet.jpg* in the base folder for a sample network architecture (it's the architecture TAs used in their implementation and is sufficient to get you pass Part 1).\n",
    "\n",
    "**TODO 4**: Do the following in ```simple_net.py```:\n",
    "- Initialize ```self.cnn_layers```\n",
    "- Initialize ```self.fc_layers```\n",
    "- Write the forward function\n",
    "\n",
    "Leave the ```self.loss_criterion = None``` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jvVL-ap0BpFx",
    "outputId": "0d88c02f-c5fd-475b-85f0-b5b2974be267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your SimpleNet architecture:  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing your SimpleNet architecture: \", verify(test_simple_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_YLUulTTdkX"
   },
   "source": [
    "### 2.2 Output prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNBpN4ofTdkZ"
   },
   "source": [
    "Let's see what out model's forward function produces for a sample input, and how it relates to classification. Pytorch's convolution and FC layers are initialized with random weights. So we should not expect any useful output without any training.\n",
    "\n",
    "We will use a data-point from the dataloader we have already created and run the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IW3f_SgTdkc"
   },
   "outputs": [],
   "source": [
    "simple_model = SimpleNet()\n",
    "\n",
    "image_loader = ImageLoader(data_base_path, \n",
    "                           split='train', \n",
    "                           transform=get_fundamental_transforms(inp_size, dataset_mean, dataset_std)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyQDxArjTdkg"
   },
   "outputs": [],
   "source": [
    "# get the 0th sample\n",
    "sample_image, sample_label = next(iter(image_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "myJ9kcqUTdkm",
    "outputId": "73d71e78-a995-4a32-88f1-76fd820e08b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape =  torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print('Input image shape = ', sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "bvHtwSATTdkr",
    "outputId": "05eaeff6-be06-464b-bfc9-f9ec991b4680"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show the image\n",
    "fig, axs = plt.subplots()\n",
    "axs.imshow(sample_image.squeeze().numpy(), cmap='gray')\n",
    "axs.axis('off')\n",
    "axs.set_title('Sample image (Target label = {})'.format(sample_label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0XEqQaNTdkw"
   },
   "outputs": [],
   "source": [
    "# run the image through the model\n",
    "sample_model_output = simple_model(sample_image.unsqueeze(0)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2v9WUSlbTdkz",
    "outputId": "fe79ea32-48c8-4ad7-8428-80b7af1651c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0946, -0.0969,  0.0243, -0.4379,  0.0314,  0.2377, -0.1942, -0.4724,\n",
      "         -0.3004, -0.2747, -0.3166, -0.3913, -0.2652, -0.2118, -0.0514]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCtiLVKKTdk3"
   },
   "source": [
    "We have a 15-dimensional tensor as output, but how does it relate to classification?\n",
    "\n",
    "We first convert the this tensor into a probability distribution over 15 classes by applying the [Softmax](https://en.wikipedia.org/wiki/Softmax_function) operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ssu7W4jTdk4"
   },
   "outputs": [],
   "source": [
    "sample_probability_values = torch.nn.functional.softmax(sample_model_output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "4BXWrwl0Tdk9",
    "outputId": "6d1791e3-de6b-4bca-ab53-6b2629faa0f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0718, 0.0717, 0.0809, 0.0510, 0.0815, 0.1001, 0.0650, 0.0492, 0.0585,\n",
      "         0.0600, 0.0575, 0.0534, 0.0606, 0.0639, 0.0750]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_probability_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTABRuGpTdlD"
   },
   "source": [
    "The prediction of the model will be the index where the probability distribution is the maximum. Convince yourself that the argmax-operation on *sample_model_values* is the same as the argmax-operation on *sample_probability_values*.\n",
    "\n",
    "**TODO 5:** Complete the ```predict_labels()``` function in ```dl_utils.py```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3j6LNUyTdlD"
   },
   "source": [
    "## 3 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2mIs5sQTdlE"
   },
   "source": [
    "We have written a model which takes in a tensor for an image and produces a 15 dimensional output for it. We saw in the previous section on how the output relates to the prediction and probability distribution. But how do we quantify the performance of the model, and how do we use that quantification to form an objective function which we can minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1esn_cCXTdlE"
   },
   "source": [
    "Ideally, we would want the probability function to have value 1 for the target *sample_label* and value 0 for the remaining class indices. To penalize the deviation between the desired probability distribution and the model-predicted distrtibution, we use the KL-divergence loss or the cross-entropy loss. Please refer to [this stackexchange post](https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation) for a good explanation and derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvSEgfOBTdlF"
   },
   "source": [
    "**TODO 6:** Assign a loss function to ```self.loss_criterion``` in ```simple_net.py```. Note that we have not done a softmax operation in the model's forward function and choose the [appropriate loss function](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "**TODO 7:** Complete the ```compute_loss()``` function in ```dl_utils.py``` to use the model's loss criterion and compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9Y3TEv7TdlG"
   },
   "outputs": [],
   "source": [
    "simple_model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Y2Lu0S_ETdlK",
    "outputId": "8f9c4f93-604b-4d87-9ff3-b6cef8496031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten()\n",
      "    (1): Linear(in_features=500, out_features=200, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=200, out_features=15, bias=True)\n",
      "  )\n",
      "  (loss_criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "hbcEPDN0TdmF",
    "outputId": "f3ed9dc9-ac9e-46a2-9916-4f4cef2dc417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-37210309bed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing your model prediction: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predict_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing your loss values: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_compute_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/computervision/proj2_release/proj2_code/proj2_unit_tests/test_base.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \"\"\"\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\\x1b[32m\\\"Correct\\\"\\x1b[0m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/computervision/proj2_release/proj2_code/proj2_unit_tests/test_dl_utils.py\u001b[0m in \u001b[0;36mtest_predict_labels\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "print(\"Testing your model prediction: \", verify(test_predict_labels))\n",
    "print(\"Testing your loss values: \", verify(test_compute_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKQCj596TdlQ"
   },
   "source": [
    "## 4 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CB0oPQtVTdlR"
   },
   "source": [
    "### 4.1 Manual gradient descent using Pytorch's autograd\n",
    "\n",
    "Till now, we have defined the model, and designed a loss function which is a proxy for *good* classification. We now to optimize the weights of the network so that the loss function is minimized.\n",
    "\n",
    "Pytorch is a very useful library for deep learning because a lot of tensor operations and functions support the flow of gradients. This feature is called [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). This functionality lets use use gradient based optimization techniques like gradient descent without writing a lot of code.\n",
    "\n",
    "Let us first understand how we can access the gradients.\n",
    "\n",
    "### Define a model and a loss function\n",
    "Suppose we have a simple objective function that looks like:\n",
    "$$ L(w) =  w^2 - 10w + 25 $$\n",
    "\n",
    "This is a convex problem, and we know that the loss $L$ is minimized for $w=5$, and we can obtain this in closed form.\n",
    "\n",
    "But let us use gradient descent to obtain the solution in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLu-7PG1TdlS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "def quadratic_loss(w: tensor) -> tensor:\n",
    "    assert w.shape==(1,)\n",
    "\n",
    "    # loss function\n",
    "    L = torch.pow(w, 2) - 10 * w + 25\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPVWOjo6TdlW"
   },
   "source": [
    "Let's compute the loss at w = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pbVokXpUTdlX",
    "outputId": "e5fc23fb-f3dc-4c14-f91a-0397c04dac21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=0.0000\tLoss=25.0000\n"
     ]
    }
   ],
   "source": [
    "w = tensor([0.0], requires_grad=True)\n",
    "\n",
    "loss = quadratic_loss(w)\n",
    "\n",
    "print('w={:.4f}\\tLoss={:.4f}'.format(w.detach().numpy().item(), loss.detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AabZ-6MnTdla"
   },
   "source": [
    "Now we can do a backward pass of the gradients to get the gradient of loss w.r.t w. Now we need to calculate the gradients with regard to the weights and biases using backprop. It will be very painful if we do it manually, but thankfully, in PyTorch we can have everything covered with autograd, which only needs a simple call of **.backward()** on our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jcrIh9P3Tdlb",
    "outputId": "b50fcd7e-cf26-424b-fd30-0bbfae18f7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.])\n"
     ]
    }
   ],
   "source": [
    "# perform backward on loss (we need to retain graph here otherwise Pytorch will throw it away)\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(w.grad.data)\n",
    "\n",
    "# manually zero out the gradient\n",
    "w.grad.zero_()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAPF2cBATdlf"
   },
   "source": [
    "Does this gradient match with the one you compute manually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmd3JnxzTdli"
   },
   "source": [
    "With the gradients, we can update the weights and biases using gradient descent:\n",
    "$$w_{k+1}=w_{k} - \\alpha\\frac{\\partial L}{\\partial w_k}$$\n",
    "where $w$ is the parameter we are updating, $\\alpha$ is the learning rate, and $\\frac{\\partial L}{\\partial w_k}$ is the gradient at step $k$. You can learn more about gradient descent [here](https://en.wikipedia.org/wiki/Gradient_descent) and [here](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIhMyeflTdlj"
   },
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "lr = .03\n",
    "\n",
    "def gradientDescentStep(w: tensor, L: tensor, lr: float=1e-3) -> None:\n",
    "    '''\n",
    "    Take a step of the gradient descent\n",
    "    '''\n",
    "    \n",
    "    # manually zero out the gradient\n",
    "    w.grad.zero_()\n",
    "\n",
    "    # perform backward on loss (we need to retain graph here otherwise Pytorch will throw it away)\n",
    "    L.backward(retain_graph=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMEZ9eHuTdlm"
   },
   "source": [
    "Let's take one step of the gradient descent and check if the the loss value decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BuBQHZwTdln"
   },
   "outputs": [],
   "source": [
    "loss = quadratic_loss(w)\n",
    "\n",
    "gradientDescentStep(w, loss, lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yIWP3Tv4Tdlq",
    "outputId": "e0124ca7-22ef-4261-fab9-72531225ea6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=0.3000\tLoss=22.0900\n"
     ]
    }
   ],
   "source": [
    "loss = quadratic_loss(w)\n",
    "print('w={:.4f}\\tLoss={:.4f}'.format(w.detach().numpy().item(), loss.detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Mb4IdocTdlv"
   },
   "source": [
    "Looks like it's been optimized!\n",
    "\n",
    "Now let's run a few more updates and see where we can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "FTfmRlc0Tdlv",
    "outputId": "3407103b-0e90-4503-f7ed-940ae983c930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: w=2.3069\tLoss=7.2527\n",
      "Iteration 20: w=3.5495\tLoss=2.1040\n",
      "Iteration 30: w=4.2187\tLoss=0.6104\n",
      "Iteration 40: w=4.5792\tLoss=0.1771\n",
      "Iteration 50: w=4.7733\tLoss=0.0514\n",
      "Iteration 60: w=4.8779\tLoss=0.0149\n",
      "Iteration 70: w=4.9342\tLoss=0.0043\n",
      "Iteration 80: w=4.9646\tLoss=0.0013\n",
      "Iteration 90: w=4.9809\tLoss=0.0004\n",
      "Iteration 100: w=4.9897\tLoss=0.0001\n",
      "Iteration 110: w=4.9945\tLoss=0.0000\n",
      "Iteration 120: w=4.9970\tLoss=0.0000\n",
      "Iteration 130: w=4.9984\tLoss=0.0000\n",
      "Iteration 140: w=4.9991\tLoss=0.0000\n",
      "Iteration 150: w=4.9995\tLoss=0.0000\n",
      "Iteration 160: w=4.9997\tLoss=0.0000\n",
      "Iteration 170: w=4.9999\tLoss=0.0000\n",
      "Iteration 180: w=4.9999\tLoss=0.0000\n",
      "Iteration 190: w=5.0000\tLoss=-0.0000\n",
      "Iteration 200: w=5.0000\tLoss=0.0000\n",
      "\n",
      "optimization takes 0.020 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    loss = quadratic_loss(w)\n",
    "    if not (i+1)%10:\n",
    "        print('Iteration {}: w={:.4f}\\tLoss={:.4f}'.format(\n",
    "            i+1, w.detach().numpy().item(), loss.detach().numpy().item()))\n",
    "        \n",
    "    gradientDescentStep(w, loss, lr) \n",
    "        \n",
    "print('\\noptimization takes %0.3f seconds'%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTFEo_JMTdlz"
   },
   "source": [
    "Seems that it's doing a great job training our model! The loss now has decreased significantly to a pretty small value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMtBy2JvTdl0"
   },
   "source": [
    "### 4.2 Optimization using Pytorch's gradient descent optimizer\n",
    "\n",
    "Now let's see how we can simplify this using the `torch.optim` package from PyTorch. You can see that using optimizer from `torch.optim` package can achieve the same results with a lot less code from our side. Also, there are many features available over the vanilla gradient descent. Let's use the Stochastic Gradient Descent (SGD) optimizer available in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "YKb3ngxtTdl1",
    "outputId": "ee4ae159-fdec-4a75-b91a-f8caa99174ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: w=2.3069\tLoss=8.2081\n",
      "Iteration 20: w=3.5495\tLoss=2.3812\n",
      "Iteration 30: w=4.2187\tLoss=0.6908\n",
      "Iteration 40: w=4.5792\tLoss=0.2004\n",
      "Iteration 50: w=4.7733\tLoss=0.0581\n",
      "Iteration 60: w=4.8779\tLoss=0.0169\n",
      "Iteration 70: w=4.9342\tLoss=0.0049\n",
      "Iteration 80: w=4.9646\tLoss=0.0014\n",
      "Iteration 90: w=4.9809\tLoss=0.0004\n",
      "Iteration 100: w=4.9897\tLoss=0.0001\n",
      "Iteration 110: w=4.9945\tLoss=0.0000\n",
      "Iteration 120: w=4.9970\tLoss=0.0000\n",
      "Iteration 130: w=4.9984\tLoss=0.0000\n",
      "Iteration 140: w=4.9991\tLoss=-0.0000\n",
      "Iteration 150: w=4.9995\tLoss=0.0000\n",
      "Iteration 160: w=4.9997\tLoss=0.0000\n",
      "Iteration 170: w=4.9999\tLoss=0.0000\n",
      "Iteration 180: w=4.9999\tLoss=0.0000\n",
      "Iteration 190: w=5.0000\tLoss=0.0000\n",
      "Iteration 200: w=5.0000\tLoss=0.0000\n",
      "\n",
      "optimization takes 0.022 seconds\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# define parameters we want to optimize\n",
    "w = tensor([0.0], requires_grad=True)\n",
    "\n",
    "optimizer = SGD([w], lr=lr)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    loss = quadratic_loss(w)   \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not (i+1)%10:\n",
    "        print('Iteration {}: w={:.4f}\\tLoss={:.4f}'.format(\n",
    "            i+1, w.detach().numpy().item(), loss.detach().numpy().item()))\n",
    "        \n",
    "print('\\noptimization takes %0.3f seconds'%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IsnddUCTdl5"
   },
   "source": [
    "### 4.3 Setting up the optimizer for SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxeVYy1bTdl6"
   },
   "source": [
    "**TODO 8:** **initialize the following cell with proper values for learning rate and weight decay** \n",
    "\n",
    "**Note:** There is nothing to do in this TODO for the first pass. You'll train the model with these values and it will be bad. Then you can come back here and tune the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2cwtK5PBpF7"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there. These values are intentionally bad.\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"AdamW\",\n",
    "  \"lr\": 1e-4,\n",
    "  \"weight_decay\": 1e-2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3pyzj8KTdl_"
   },
   "source": [
    "We will now set up a utility function to define an optimizer on the loss for a model.\n",
    "\n",
    "**TODO 9:** complete the ```get_optimizer()``` function in ```optimizer.py```. The helper function accepts three basic configurations as defined below. Any other configuration is optional. *SGD* optimizer type should be supported, anything else is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0CrYZa4BpGE"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(simple_model, optimizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3b8cjmrSTdmK"
   },
   "source": [
    "## 5 Training SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBrJnj2tTdmL"
   },
   "source": [
    "We have completed all the components required to train the first model for this course. Let's pass in the model architecture, optimizer, transforms for both the training and testing datasets into the trainer, and proceed to the next cell to train it. If you have implemented everything correctly, you should be seeing a decreasing loss value.\n",
    "\n",
    "**Note** in this project, we will be using the test set as the validation set (i.e. using it to guide our decisions about models and hyperparamters while training. In actual practise, you would not interact with the test set until reporting the final results.\n",
    "\n",
    "**Note** that your CPU should be sufficient to handle the training process for all networks in this project, and the following training cells will take less than 5 minutes; you may also want to decrease the value for `num_epochs` and quickly experiment with your parameters. The default value of **30** is good enough to get you around the threshold for Part 1, and you are free to increase it a bit and adjust other parameters in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiGOvPJfBpGO"
   },
   "outputs": [],
   "source": [
    "# re-init the model so that the weights are all random\n",
    "simple_model = SimpleNet()\n",
    "optimizer = get_optimizer(simple_model, optimizer_config)\n",
    "\n",
    "trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = simple_model,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = os.path.join(model_base_path, 'simple_net'),\n",
    "                  train_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "paNLyU5cBpGX",
    "outputId": "e3620552-cc23-4ad4-c2c5-fe58aae544a8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jv1T8xv2TdmR"
   },
   "source": [
    "Now let's have your model predict on some examples and see how well it performs qualitatively. You should try the following cell multiple times to understand whats happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thCGob3JTdmR"
   },
   "outputs": [],
   "source": [
    "# visualize train split\n",
    "print(\"Examples from train split:\")\n",
    "visualize(simple_model, 'train', get_fundamental_transforms(inp_size, dataset_mean, dataset_std), data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQbkZhjlTdmU"
   },
   "outputs": [],
   "source": [
    "# visualize test split\n",
    "print(\"Examples from test split:\")\n",
    "visualize(simple_model, 'test', get_fundamental_transforms(inp_size, dataset_mean, dataset_std), data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "z0b_WwJhBpGf",
    "outputId": "40ba0241-2b98-4262-d58a-afb8535e5d7b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8epn0IBmBpGn",
    "outputId": "f0e73de0-ac15-4f5c-b4d9-4baabd32b670",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {:.4f}; Validation Accuracy = {:.4f}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing simple net weights saved: ', verify(test_simple_net_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWNCMmRzTdmc"
   },
   "source": [
    "After you have finished the training process, now plot out the loss and accuracy history. You can also check out the final accuracy for both training and validation data. Copy the accuracy plots and values onto the report, and answer the questions there. \n",
    "\n",
    "**TODO 10:** Obtain a **45%** validation accuracy to receive full credits for Part 1. You can go back to TODO 8 first to tune your paramters for optimization using the following tips:\n",
    "\n",
    "**Tips**:\n",
    "1. If the loss decreases very slowly, try increasing the value of the lr (learning rate).\n",
    "2. Initially keep the value of weight decay (L2-regulization) very low.\n",
    "3. Try to first adjust lr in multiples of 3 initially. When you are close to reasonable performance, do a more granular adjustment.\n",
    "4. If you want to increase the validation accuracy by a little bit, try increasing the weight_decay to prevent overfitting. Do not use tricks from Section 6 just yet.\n",
    "\n",
    "If you still need to tweak the model architecture, you are free to do so. But remember complex models will require more time to train, and TAs could achieve ~50% accuracy with the described model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaopDcToTdnj"
   },
   "source": [
    "## Code testing\n",
    "We have provided a set of tests for you to evaluate your implementation. We have included tests inside ```proj2.ipynb``` so you can check your progress as you implement each section. At the end, you should call the tests from the terminal using the command ```pytest proj2_code/proj2_unit_tests/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "This is very important as you will lose 5 points for every time you do not follow the instructions.\n",
    "\n",
    "Do install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config files we provide you, so anything that's not in there by default will probably cause your code to break during grading. Do use absolute paths in your code or your code will break. Use relative paths like the starter code already does. Failure to follow any of these instructions will lead to point deductions. Create the zip file using ```python zip_submission.py --gt_username <your_gt_username>``` (it will zip up the appropriate directories/files for you!) and hand it through Gradescope. Remember to submit your report as a PDF to Gradescope as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "| Test name | Scores |\n",
    "| --------- | ------ |\n",
    "| ```test_mean_and_variance``` | 3 |\n",
    "| ```test_fundamental_transforms``` | 3 |\n",
    "| ```test_dataset_length``` | 2 |\n",
    "| ```test_class_values``` | 3 | \n",
    "| ```test_unique_values``` | 3 |\n",
    "| ```test_load_img_from_path``` | 3 |\n",
    "| ```test_predict_labels``` | 4 |\n",
    "| ```test_compute_loss``` | 4 |\n",
    "| ```test_simple_net``` | 8 |\n",
    "| ```test_simple_net_checkpoint``` | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit\n",
    "\n",
    "Assignment developed by Ayush Baid, Haoxin Ma, Jing Wu and Frank Dellaert, based on a the original assigment by Ayush Baid, Cusuh Ham, Jonathan Balloch, Shenhao Jiang, Frank Dellaert, and James Hays."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "proj2_sample.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "34faafbca48d4998a5f52ae5f6149970": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "522e462c7714471781b76798e8f9e824": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69d842f91bc14493b3e467c7d2288d33": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f5b6897ec8b4890943fd97564939249": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a647ea75a3724ea8b901ab757a649fb9",
      "placeholder": "",
      "style": "IPY_MODEL_522e462c7714471781b76798e8f9e824",
      "value": " 233M/233M [02:15&lt;00:00, 1.80MB/s]"
     }
    },
    "8fec6fdac9cf4ab5bdf5049e5915668b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f31149d3b9b14b4691046a82a45ccf5c",
       "IPY_MODEL_6f5b6897ec8b4890943fd97564939249"
      ],
      "layout": "IPY_MODEL_b2cd8b942e294970bf504273c129496c"
     }
    },
    "a647ea75a3724ea8b901ab757a649fb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2cd8b942e294970bf504273c129496c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f31149d3b9b14b4691046a82a45ccf5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69d842f91bc14493b3e467c7d2288d33",
      "max": 244418560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_34faafbca48d4998a5f52ae5f6149970",
      "value": 244418560
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
